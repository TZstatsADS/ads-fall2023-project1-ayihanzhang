knitr::opts_chunk$set(echo = TRUE)
library(tm)
library(tidytext)
library(tidyverse)
library(DT)
urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/cleaned_hm.csv'
hm_data <- read_csv(urlfile)
corpus <- VCorpus(VectorSource(hm_data$cleaned_hm))%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map(removeNumbers)%>%
tm_map(removeWords, character(0))%>%
tm_map(stripWhitespace)
stemmed <- tm_map(corpus, stemDocument) %>%
tidy() %>%
select(text)
dict <- tidy(corpus) %>%
select(text) %>%
unnest_tokens(dictionary, text)
data("stop_words")
word <- c("happy","ago","yesterday","lot","today","months","month",
"happier","happiest","last","week","past")
stop_words <- stop_words %>%
bind_rows(mutate(tibble(word), lexicon = "updated"))
completed <- stemmed %>%
mutate(id = row_number()) %>%
unnest_tokens(stems, text) %>%
bind_cols(dict) %>%
anti_join(stop_words, by = c("dictionary" = "word"))
completed <- completed %>%
group_by(stems) %>%
count(dictionary) %>%
mutate(word = dictionary[which.max(n)]) %>%
ungroup() %>%
select(stems, word) %>%
distinct() %>%
right_join(completed) %>%
select(-stems)
completed <- completed %>%
group_by(id) %>%
summarise(text = str_c(word, collapse = " ")) %>%
ungroup()
hm_data <- hm_data %>%
mutate(id = row_number()) %>%
inner_join(completed)
datatable(hm_data)
write_csv(hm_data, "../output/processed_moments.csv")
library(word2vec)
library(tidytext)
library(dplyr)
library(ggplot2)
library(ggrepel)
library(uwot)
library(word2vec)
library(tidytext)
library(dplyr)
library(ggplot2)
library(ggrepel)
library(uwot)
install.packages("uwot")
library(word2vec)
library(tidytext)
library(dplyr)
library(ggplot2)
library(ggrepel)
library(uwot)
library(udpipe)
library(word2vec)
library(tidytext)
library(dplyr)
library(ggplot2)
library(ggrepel)
library(uwot)
library(udpipe)
gtwd()
getwd()
cleaned_df<-read_csv("/Users/fiona/Documents/GitHub/ads-fall2023-project1-ayihanzhang/data/cleaned_hm.csv")
cleaned_df<-read_csv("/Users/fiona/Documents/GitHub/ads-fall2023-project1-ayihanzhang/data/cleaned_hm.csv")
demo_df<-read_csv("/Users/fiona/Documents/GitHub/ads-fall2023-project1-ayihanzhang/data/demographic.csv")
library(word2vec)
library(tidytext)
library(dplyr)
library(ggplot2)
library(ggrepel)
library(uwot)
library(udpipe)
library(readr)
cleaned_df<-read_csv("/Users/fiona/Documents/GitHub/ads-fall2023-project1-ayihanzhang/data/cleaned_hm.csv")
demo_df<-read_csv("/Users/fiona/Documents/GitHub/ads-fall2023-project1-ayihanzhang/data/demographic.csv")
View(cleaned_df)
View(demo_df)
# Load 2 data set required
cleaned_df<-read_csv("/Users/fiona/Documents/GitHub/ads-fall2023-project1-ayihanzhang/data/cleaned_hm.csv")
demo_df<-read_csv("/Users/fiona/Documents/GitHub/ads-fall2023-project1-ayihanzhang/data/demographic.csv")
# merge 2 data set
merged_df<-merge(cleaned_df,demo_df)
library(word2vec)
library(tidytext)
library(dplyr)
library(ggplot2)
library(ggrepel)
library(uwot)
library(udpipe)
library(readr)
library(tm)
library(topicmodels)
install.packages("topicmodels")
library(word2vec)
library(tidytext)
library(dplyr)
library(ggplot2)
library(ggrepel)
library(uwot)
library(udpipe)
library(readr)
library(tm)
library(no)
library(word2vec)
library(tidytext)
library(dplyr)
library(ggplot2)
library(ggrepel)
library(uwot)
library(udpipe)
library(readr)
library(tm)
library(topicmodels)
# Data preprocessing
corpus <- Corpus(VectorSource(merged_df$cleaned_hm))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords("en"))
corpus <- tm_map(corpus, stemDocument)
# Create Document-Term Matrix (DTM)
dtm <- DocumentTermMatrix(corpus)
# Build LDA model
lda_model <- LDA(dtm, k = 5, control = list(seed = 1234))
View(dtm)
# Build LDA model
lda_model <- LDA(dtm, k = 5, control = list(seed = 1234))
?LDA())
?LDA()
# Build LDA model
lda_model <- LDA(dtm, k = 10, control = list(seed = 1234))
# Data preprocessing
corpus <- Corpus(VectorSource(merged_df$cleaned_hm))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords("en"))
corpus <- tm_map(corpus, stemDocument)
# Create Document-Term Matrix (DTM)
dtm <- DocumentTermMatrix(corpus)
# Remove rows with all zeros (i.e., empty documents)
dtm <- dtm[row_sums(as.matrix(dtm)) > 0, ]
# Data preprocessing
corpus <- Corpus(VectorSource(merged_df$cleaned_hm))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords("en"))
corpus <- tm_map(corpus, stemDocument)
# Create Document-Term Matrix (DTM)
dtm <- DocumentTermMatrix(corpus)
# Remove rows with all zeros (i.e., empty documents)
dtm <- dtm[rowSums(as.matrix(dtm)) > 0, ]
# Build LDA model
lda_model <- LDA(dtm, k = 5, control = list(seed = 1234))
# Data preprocessing
corpus <- VCorpus(VectorSource(hm_data$cleaned_hm))%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map(removeNumbers)%>%
tm_map(removeWords, character(0))%>%
tm_map(stripWhitespace)
# Data preprocessing
corpus <- VCorpus(VectorSource(cleaned_df$cleaned_hm))%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map(removeNumbers)%>%
tm_map(removeWords, character(0))%>%
tm_map(stripWhitespace)
stemmed <- tm_map(corpus, stemDocument) %>%
tidy() %>%
select(text)
# Create Document-Term Matrix (DTM)
dtm <- DocumentTermMatrix(stemmed)
# Remove rows with all zeros (i.e., empty documents)
# dtm <- dtm[rowSums(as.matrix(dtm)) > 0, ]
# Build LDA model
lda_model <- LDA(dtm, k = 5, control = list(seed = 1234))
# Data preprocessing
corpus <- VCorpus(VectorSource(cleaned_df$cleaned_hm))%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map(removeNumbers)%>%
tm_map(removeWords, character(0))%>%
tm_map(stripWhitespace)%>%
tm_map(corpus, stemDocument)
# Data preprocessing
corpus <- VCorpus(VectorSource(cleaned_df$cleaned_hm))%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map(removeNumbers)%>%
tm_map(removeWords, character(0))%>%
tm_map(stripWhitespace)%>%
tm_map(corpus, stemDocument)
# Data preprocessing
corpus <- VCorpus(VectorSource(cleaned_df$cleaned_hm))%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map(removeNumbers)%>%
tm_map(removeWords, character(0))%>%
tm_map(stripWhitespace)
stemmed <- tm_map(corpus, stemDocument) %>%
tidy() %>%
select(text)
# Create Document-Term Matrix (DTM)
dtm <- DocumentTermMatrix(corpus)
# Remove rows with all zeros (i.e., empty documents)
# dtm <- dtm[rowSums(as.matrix(dtm)) > 0, ]
# Build LDA model
lda_model <- LDA(dtm, k = 5, control = list(seed = 1234))
# Display topics
topics <- as.data.frame(lda_model@beta)
top_n_words <- 5
top_terms <- apply(topics, MARGIN = 2, FUN = function(x) { names(sort(x, decreasing = TRUE)[1:top_n_words]) })
print(top_terms)
# Load 2 data set required
cleaned_df<-read_csv("/Users/fiona/Documents/GitHub/ads-fall2023-project1-ayihanzhang/output/processed_moments.csv")
demo_df<-read_csv("/Users/fiona/Documents/GitHub/ads-fall2023-project1-ayihanzhang/data/demographic.csv")
# merge 2 data set
merged_df<-merge(cleaned_df,demo_df)
View(cleaned_df)
# Data preprocessing
corpus <- VCorpus(VectorSource(cleaned_df$text))%>%
tm_map(stripWhitespace)
# Create Document-Term Matrix (DTM)
dtm <- DocumentTermMatrix(corpus)
# Remove rows with all zeros (i.e., empty documents)
# dtm <- dtm[rowSums(as.matrix(dtm)) > 0, ]
# Build LDA model
lda_model <- LDA(dtm, k = 5, control = list(seed = 1234))
# Remove rows with all zeros (i.e., empty documents)
dtm <- dtm[rowSums(as.matrix(dtm)) > 0, ]
# Load 2 data set required
cleaned_df<-read_csv("/Users/fiona/Documents/GitHub/ads-fall2023-project1-ayihanzhang/data/cleaned_hm.csv")
demo_df<-read_csv("/Users/fiona/Documents/GitHub/ads-fall2023-project1-ayihanzhang/data/demographic.csv")
# merge 2 data set
merged_df<-merge(cleaned_df,demo_df)
# Convert all characters into lower cases
lower_text<- tolower(cleaned_data$cleaned_hm)
# Convert all characters into lower cases
lower_text<- tolower(cleaned_df$cleaned_hm)
# Convert all characters into lower cases
lower_text<- tolower(merged_df$cleaned_hm)
View(merged_df)
# Load 2 data set required
cleaned_df<-read_csv("/Users/fiona/Documents/GitHub/ads-fall2023-project1-ayihanzhang/data/cleaned_hm.csv")
demo_df<-read_csv("/Users/fiona/Documents/GitHub/ads-fall2023-project1-ayihanzhang/data/demographic.csv")
# merge 2 data set
merged_df<-merge(cleaned_df,demo_df)
df<-merged_df%>%select(cleaned_hm, parenthood)
View(df)
# Convert all characters into lower cases
lower_text<- tolower(df$cleaned_hm)
# This function is referred to the process_data function in tutorial by Claire He
process_text <- function(x, n_topics){
anno <- udpipe(x, "english", trace = 5000, parallel.cores = 1)
anno <- subset(anno, !is.na(lemma) & nchar(lemma) > 1 & !upos %in% "PUNCT")
anno$text <- sprintf("%s--%s", anno$lemma, anno$upos)
x <- paste.data.frame(anno, term = "text", group = "doc_id", collapse = " ")
model <- word2vec(x = x$text, dim = n_topics, iter = 20, split = c(" ", ".\n?!"))
embedding <- as.matrix(model)
viz <- umap(embedding, n_neighbors = 15, n_threads = 2)
rownames(viz) <- rownames(embedding)
df <- data.frame(word = gsub("--.+", "", rownames(viz)),
upos = gsub(".+--", "", rownames(viz)),
x = viz[, 1], y = viz[, 2],
stringsAsFactors = FALSE)
df <- subset(df, upos %in% c("ADJ", "NOUN"))
return(list('data'=df,'model'=model))
}
l <- process_data(x, 15)
# This function is referred to the process_data function in tutorial by Claire He
process_text <- function(x, n_topics){
anno <- udpipe(x, "english", trace = 5000, parallel.cores = 1)
anno <- subset(anno, !is.na(lemma) & nchar(lemma) > 1 & !upos %in% "PUNCT")
anno$text <- sprintf("%s--%s", anno$lemma, anno$upos)
x <- paste.data.frame(anno, term = "text", group = "doc_id", collapse = " ")
model <- word2vec(x = x$text, dim = n_topics, iter = 20, split = c(" ", ".\n?!"))
embedding <- as.matrix(model)
viz <- umap(embedding, n_neighbors = 15, n_threads = 2)
rownames(viz) <- rownames(embedding)
df <- data.frame(word = gsub("--.+", "", rownames(viz)),
upos = gsub(".+--", "", rownames(viz)),
x = viz[, 1], y = viz[, 2],
stringsAsFactors = FALSE)
df <- subset(df, upos %in% c("ADJ", "NOUN"))
return(list('data'=df,'model'=model))
}
l <- process_text(lower_text, 25)
word_shopping <- predict(l$model, c("shopping//NOUN"), type = "nearest", top_n = 25)$`shopping//NOUN`$term2
word_shopping <- predict(l$model, c("shop//NOUN"), type = "nearest", top_n = 25)$`shopping//NOUN`$term2
word_shopping <- predict(l$model, c("shop//VERB"), type = "nearest", top_n = 25)$`shopping//NOUN`$term2
View(df)
word_shopping <- predict(l$model, c("shop//VERB"), type = "nearest", top_n = 25)$`shop//VERB`$term2
word_shopping <- predict(l$model, c("shop//VERB"), type = "nearest", top_n = 25)$`shop//NOUN`$term2
word_shopping <- predict(l$model, c("shop//NOUN"), type = "nearest", top_n = 25)$`shop//NOUN`$term2
#word_shopping <- predict(l$model, c("shop//NOUN"), type = "nearest", top_n = 25)$`shop//NOUN`$term2
word_happiness <- predict(l$model, c("happy//ADJ"), type = "nearest", top_n = 50)$`happy//ADJ`$term2
word_shopping <- predict(l$model, c("shopping--NOUN"), type = "nearest", top_n = 25)$`shopping--NOUN`$term2
word_happiness <- predict(l$model, c("happy--ADJ"), type = "nearest", top_n = 50)$`happy--ADJ`$term2
word_shopping
word_shopping <- predict(l$model, c("shopping--NOUN"), type = "nearest", top_n = 25)$`shopping--NOUN`$term2
word_happiness <- predict(l$model, c("happy--ADJ"), type = "nearest", top_n = 50)$`happy--ADJ`$term2
shopping_df <- subset(l$data, rownames(l$data) %in% word_shopping)
happiness_df <- subset(l$data, rownames(l$data) %in% word_happiness)
View(shopping_df)
options(ggrepel.max.overlaps=Inf)
ggplot(shopping_df, aes(x=x, y=y, label=word, color='blue')) + geom_text_repel() +
geom_text_repel(data=happiness_df, aes(x=x, y=y, label=word, color='red'))
labs(title = "100 most similar words to animal with word2vec - umap")
save.image('/Users/fiona/Documents/GitHub/ads-fall2023-project1-ayihanzhang/figs/general_w2v.png')
options(ggrepel.max.overlaps=Inf)
ggplot(shopping_df, aes(x=x, y=y, label=word, color='blue')) + geom_text_repel() +
geom_text_repel(data=happiness_df, aes(x=x, y=y, label=word, color='red')) +
labs(title = "100 most similar words to animal with word2vec - umap")
save.image('/Users/fiona/Documents/GitHub/ads-fall2023-project1-ayihanzhang/figs/general_w2v.png')
word_shopping <- predict(l$model, c("shopping--NOUN"), type = "nearest", top_n = 25)$`shopping--NOUN`$term2
word_happiness <- predict(l$model, c("happy--ADJ"), type = "nearest", top_n = 50)$`happy--ADJ`$term2
shopping_df <- subset(l$data, rownames(l$data) %in% word_shopping)
happiness_df <- subset(l$data, rownames(l$data) %in% word_happiness)
options(ggrepel.max.overlaps=Inf)
ggplot(shopping_df, aes(x=x, y=y, label=word, color='blue')) + geom_text_repel() +
geom_text_repel(data=happiness_df, aes(x=x, y=y, label=word, color='red')) +
labs(title = "100 most similar words to animal with word2vec - umap")
save.image('/Users/fiona/Documents/GitHub/ads-fall2023-project1-ayihanzhang/figs/general_w2v.png')
View(shopping_df)
options(ggrepel.max.overlaps = Inf)
ggplot(shopping_df, aes(x = x, y = y, label = word, color='blue')) +
geom_text_repel() + theme_void() + geom_text_repel(data=happiness_df, aes(x=x, y=y, label=word, color='red'))
labs(title = "100 most similar words to animal with word2vec - umap")
#save.image('/Users/fiona/Documents/GitHub/ads-fall2023-project1-ayihanzhang/figs/general_w2v.png')
library(word2vec)
library(tidytext)
library(dplyr)
library(ggplot2)
library(ggrepel)
library(uwot)
library(udpipe)
library(readr)
library(tm)
library(topicmodels)
options(ggrepel.max.overlaps = Inf)
ggplot(shopping_df, aes(x = x, y = y, label = word, color='blue')) +
geom_text_repel() + theme_void() + geom_text_repel(data=happiness_df, aes(x=x, y=y, label=word, color='red'))
labs(title = "100 most similar words to animal with word2vec - umap")
#save.image('/Users/fiona/Documents/GitHub/ads-fall2023-project1-ayihanzhang/figs/general_w2v.png')
options(ggrepel.max.overlaps = Inf)
ggplot(shopping_df, aes(x = x, y = y, label = word, color='blue')) +
geom_text_repel()
#save.image('/Users/fiona/Documents/GitHub/ads-fall2023-project1-ayihanzhang/figs/general_w2v.png')
options(ggrepel.max.overlaps = Inf)
ggplot(shopping_df, aes(x = x, y = y, label = word, color='blue'))
#save.image('/Users/fiona/Documents/GitHub/ads-fall2023-project1-ayihanzhang/figs/general_w2v.png')
options(ggrepel.max.overlaps = Inf)
ggplot(shopping_df, aes(x = x, y = y, label = word, color='blue')) + geom_point()
#save.image('/Users/fiona/Documents/GitHub/ads-fall2023-project1-ayihanzhang/figs/general_w2v.png')
options(ggrepel.max.overlaps = Inf)
ggplot(animal_df, aes(x = x, y = y, label = word, color='blue')) +
geom_text_repel() + theme_void() + geom_text_repel(data=happiness_df, aes(x=x, y=y, label=word, color='red'))
#options(ggrepel.max.overlaps = Inf)
ggplot(shopping_df, aes(x = x, y = y, label = word, color='blue')) +
geom_text_repel() + theme_void() + geom_text_repel(data=happiness_df, aes(x=x, y=y, label=word, color='red'))
labs(title = "100 most similar words to animal with word2vec - umap")
#save.image('/Users/fiona/Documents/GitHub/ads-fall2023-project1-ayihanzhang/figs/general_w2v.png')
#options(ggrepel.max.overlaps = Inf)
ggplot(shopping_df, aes(x = x, y = y, label = word, color='blue')) +
geom_text_repel() +
theme_void() +
geom_text_repel(data=happiness_df, aes(x=x, y=y, label=word, color='red'))
labs(title = "100 most similar words to animal with word2vec - umap")
#save.image('/Users/fiona/Documents/GitHub/ads-fall2023-project1-ayihanzhang/figs/general_w2v.png')
#options(ggrepel.max.overlaps = Inf)
P <- ggplot(shopping_df, aes(x = x, y = y, label = word, color='blue')) +
geom_text_repel() +
theme_void() +
geom_text_repel(data=happiness_df, aes(x=x, y=y, label=word, color='red'))
labs(title = "100 most similar words to animal with word2vec - umap")
#save.image('/Users/fiona/Documents/GitHub/ads-fall2023-project1-ayihanzhang/figs/general_w2v.png')
print(P)
#options(ggrepel.max.overlaps = Inf)
P_general <- ggplot(shopping_df, aes(x=x, y=y, label=word, color='blue'))+
geom_text_repel()+
geom_text_repel(data=happiness_df, aes(x=x, y=y, label=word, color='red'))+
labs(title = "100 most similar words to animal with word2vec - umap")
print(P_general)
#options(ggrepel.max.overlaps = Inf)
P_general <- ggplot(data=shopping_df, aes(x=x, y=y, label=word, color='blue'))+
geom_text_repel()+
geom_text_repel(data=happiness_df, aes(x=x, y=y, label=word, color='red'))+
labs(title = "100 most similar words to animal with word2vec - umap")
print(P_general)
#save.image('/Users/fiona/Documents/GitHub/ads-fall2023-project1-ayihanzhang/figs/general_w2v.png')
#options(ggrepel.max.overlaps = Inf)
P_general <- ggplot(data=shopping_df, aes(x=x, y=y, label=word, color='blue'))+
geom_text_repel()+
geom_text_repel(data=happiness_df, aes(x=x, y=y, label=word, color='red'))
print(P_general)
#save.image('/Users/fiona/Documents/GitHub/ads-fall2023-project1-ayihanzhang/figs/general_w2v.png')
View(P_general)
install.packages("ggplot2")
install.packages("ggplot2")
install.packages("ggplot2")
install.packages("ggplot2")
#options(ggrepel.max.overlaps = Inf)
P_general <- ggplot(data=shopping_df, aes(x=x, y=y, label=word, color='blue'))+
geom_text_repel()+
geom_text_repel(data=happiness_df, aes(x=x, y=y, label=word, color='red'))
library(word2vec)
library(tidytext)
library(dplyr)
library(ggplot2)
library(ggrepel)
library(uwot)
library(udpipe)
library(readr)
library(tm)
library(topicmodels)
#options(ggrepel.max.overlaps = Inf)
P_general <- ggplot(data=shopping_df, aes(x=x, y=y, label=word, color='blue'))+
geom_text_repel()+
geom_text_repel(data=happiness_df, aes(x=x, y=y, label=word, color='red'))
print(P_general)
#save.image('/Users/fiona/Documents/GitHub/ads-fall2023-project1-ayihanzhang/figs/general_w2v.png')
word_shopping <- predict(l$model, c("shopping--NOUN"), type = "nearest", top_n = 25)$`shopping--NOUN`$term2
library(word2vec)
library(tidytext)
library(dplyr)
library(ggplot2)
library(ggrepel)
library(uwot)
library(udpipe)
library(readr)
library(tm)
library(topicmodels)
# Load 2 data set required
cleaned_df<-read_csv("/Users/fiona/Documents/GitHub/ads-fall2023-project1-ayihanzhang/data/cleaned_hm.csv")
# Load 2 data set required
cleaned_df<-read_csv("/Users/fiona/Documents/GitHub/ads-fall2023-project1-ayihanzhang/data/cleaned_hm.csv")
library(word2vec)
library(tidytext)
library(dplyr)
library(ggplot2)
library(ggrepel)
library(uwot)
library(udpipe)
library(readr)
library(tm)
library(topicmodels)
# Load 2 data set required
cleaned_df<-read_csv("/Users/fiona/Documents/GitHub/ads-fall2023-project1-ayihanzhang/data/cleaned_hm.csv")
